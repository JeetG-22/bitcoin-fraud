{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a500e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import platform\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "project_root = os.environ.get(\"PROJECT_ROOT\")\n",
    "sys.path.append('../models/graphCL')\n",
    "\n",
    "from models import GCL_Encoder, LogisticRegression\n",
    "from augmentations import TemporalAugmentor\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GRAPH CONTRACTION SCRIPT ---\n",
    "# Turns Wallet nodes (Tx -> Addr -> Tx) into direct edges (Tx -> Tx).\n",
    "\n",
    "ELLIPTIC_PP_DIR = os.path.join(project_root, \"elliptic++_bitcoin_dataset\")\n",
    "\n",
    "print(f\"Data Directory: {ELLIPTIC_PP_DIR}\")\n",
    "\n",
    "# Tx -> Addr\n",
    "tx_addr_path = os.path.join(ELLIPTIC_PP_DIR, \"TxAddr_edgelist.csv\")\n",
    "print(f\"Loading {tx_addr_path}...\")\n",
    "df_tx_addr = pd.read_csv(tx_addr_path)\n",
    "df_tx_addr.columns = [\"txId_src\", \"address\"]\n",
    "\n",
    "# Addr -> Tx\n",
    "addr_tx_path = os.path.join(ELLIPTIC_PP_DIR, \"AddrTx_edgelist.csv\")\n",
    "print(f\"Loading {addr_tx_path}...\")\n",
    "df_addr_tx = pd.read_csv(addr_tx_path)\n",
    "df_addr_tx.columns = [\"address\", \"txId_dst\"]\n",
    "\n",
    "# Tx_A -> Address_X -> Tx_B  ==>  Tx_A -> Tx_B\n",
    "print(\"Performing Graph Contraction (Tx->Addr + Addr->Tx = Tx->Tx)...\")\n",
    "df_contracted = pd.merge(df_tx_addr, df_addr_tx, on=\"address\", how=\"inner\")\n",
    "\n",
    "print(f\"  - Tx->Addr edges: {len(df_tx_addr):,}\")\n",
    "print(f\"  - Addr->Tx edges: {len(df_addr_tx):,}\")\n",
    "print(f\"  - Contracted Tx->Tx edges: {len(df_contracted):,}\")\n",
    "\n",
    "features_path = os.path.join(ELLIPTIC_PP_DIR, \"txs_features.csv\")\n",
    "print(f\"Loading timestamps from {features_path}...\")\n",
    "df_features = pd.read_csv(features_path, usecols=[\"txId\", \"Time step\"])\n",
    "time_map = df_features.set_index(\"txId\")[\"Time step\"].to_dict()\n",
    "\n",
    "print(\"Mapping timestamps to edges...\")\n",
    "df_contracted[\"src_time\"] = df_contracted[\"txId_src\"].map(time_map)\n",
    "df_contracted[\"dst_time\"] = df_contracted[\"txId_dst\"].map(time_map)\n",
    "\n",
    "df_contracted = df_contracted.dropna(subset=[\"src_time\", \"dst_time\"])\n",
    "\n",
    "num_same_time = (df_contracted[\"dst_time\"] == df_contracted[\"src_time\"]).sum()\n",
    "num_forward = (df_contracted[\"dst_time\"] > df_contracted[\"src_time\"]).sum()\n",
    "num_backward = (df_contracted[\"dst_time\"] < df_contracted[\"src_time\"]).sum()\n",
    "\n",
    "print(\"\\nContracted Graph Temporal Analysis:\")\n",
    "print(f\"  Same-timestep edges: {num_same_time:,} ({100*num_same_time/len(df_contracted):.1f}%)\")\n",
    "print(f\"  Forward-causal edges: {num_forward:,} ({100*num_forward/len(df_contracted):.1f}%) <--- THIS IS WHAT WE WANTED\")\n",
    "print(f\"  Backward-invalid edges: {num_backward:,} ({100*num_backward/len(df_contracted):.3f}%)\")\n",
    "\n",
    "\n",
    "# Remove backwards edges\n",
    "valid_edges = df_contracted[df_contracted[\"dst_time\"] >= df_contracted[\"src_time\"]]\n",
    "\n",
    "output_file = os.path.join(ELLIPTIC_PP_DIR, \"elliptic_pp_contracted_edgelist.csv\")\n",
    "print(f\"\\nSaving {len(valid_edges):,} valid causal edges to {output_file}...\")\n",
    "valid_edges[[\"txId_src\", \"txId_dst\"]].to_csv(output_file, index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELLIPTIC_PP_DIR = os.path.join(project_root, \"elliptic++_bitcoin_dataset\")\n",
    "features_path = os.path.join(ELLIPTIC_PP_DIR, \"txs_features.csv\")\n",
    "classes_path = os.path.join(ELLIPTIC_PP_DIR, \"txs_classes.csv\")\n",
    "edges_path = os.path.join(ELLIPTIC_PP_DIR, \"elliptic_pp_contracted_edgelist.csv\")\n",
    "\n",
    "print(\"Loading features...\")\n",
    "df_features = pd.read_csv(features_path)\n",
    "df_features = df_features.rename(columns={\"Time step\": \"timeStep\"})\n",
    "df_features = df_features.set_index(\"txId\")\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c != \"timeStep\"]\n",
    "x = torch.tensor(df_features[feature_cols].values, dtype=torch.float)\n",
    "\n",
    "time_vals = df_features[\"timeStep\"].values\n",
    "time = torch.tensor(time_vals, dtype=torch.long)\n",
    "\n",
    "print(f\"Loading classes from {classes_path}...\")\n",
    "df_classes = pd.read_csv(classes_path).set_index(\"txId\")\n",
    "df_classes = df_classes.reindex(df_features.index)\n",
    "\n",
    "# 1=Illicit, 2=Licit, 3=Unknown -> 1, 0, -1\n",
    "class_map = {1: 1, 2: 0, 3: -1}\n",
    "y = torch.tensor(df_classes[\"class\"].map(class_map).fillna(-1).values, dtype=torch.long)\n",
    "\n",
    "tx_ids = list(df_features.index)\n",
    "node_mapping = {int(txid): idx for idx, txid in enumerate(tx_ids)}\n",
    "\n",
    "print(f\"Loading edges from {edges_path}...\")\n",
    "df_edges = pd.read_csv(edges_path)\n",
    "\n",
    "valid_src = df_edges[\"txId_src\"].isin(node_mapping)\n",
    "valid_dst = df_edges[\"txId_dst\"].isin(node_mapping)\n",
    "df_edges = df_edges[valid_src & valid_dst]\n",
    "\n",
    "print(\"Deduplicating edges (calculating weights)...\")\n",
    "df_edges_grouped = df_edges.groupby([\"txId_src\", \"txId_dst\"]).size().reset_index(name=\"weight\")\n",
    "\n",
    "print(f\"  - Original edges: {len(df_edges):,}\")\n",
    "print(f\"  - Deduplicated edges: {len(df_edges_grouped):,}\")\n",
    "\n",
    "src_idx = df_edges_grouped[\"txId_src\"].map(node_mapping).values\n",
    "dst_idx = df_edges_grouped[\"txId_dst\"].map(node_mapping).values\n",
    "edge_index = torch.tensor([src_idx, dst_idx], dtype=torch.long)\n",
    "edge_attr = torch.tensor(df_edges_grouped[\"weight\"].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "train_mask = time < 35\n",
    "test_mask = time >= 35\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, time=time, train_mask=train_mask, test_mask=test_mask)\n",
    "data.tx_id_to_node = node_mapping\n",
    "data.node_to_tx_id = {v: k for k, v in node_mapping.items()}\n",
    "data.feature_columns = feature_cols\n",
    "\n",
    "print(\"Elliptic++ Data Object Created (Hardened):\")\n",
    "print(data)\n",
    "print(\"Num nodes:\", data.num_nodes)\n",
    "print(\"Num edges:\", data.num_edges)\n",
    "print(\"Features shape:\", data.x.shape)\n",
    "print(\"Class distribution:\", torch.unique(data.y, return_counts=True))\n",
    "\n",
    "data = data.to(DEVICE)\n",
    "print(f\"Data ready: {data.num_nodes} nodes, {data.num_edges} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9279a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096\n",
    "HIDDEN_CHANNELS = 256\n",
    "OUT_CHANNELS = 128\n",
    "LR_PRETRAIN = 0.0005\n",
    "EPOCHS_PRETRAIN = 300\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "RESULTS_DIR = os.path.join(project_root, \"results\", \"graphCL\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "\n",
    "config = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"hidden_channels\": HIDDEN_CHANNELS,\n",
    "    \"out_channels\": OUT_CHANNELS,\n",
    "    \"lr_pretrain\": LR_PRETRAIN,\n",
    "    \"epochs_pretrain\": EPOCHS_PRETRAIN,\n",
    "    \"num_neighbors\": [25, 15],\n",
    "    \"temperature\": 0.1,\n",
    "    \"intra_step_drop_prob\": 0.5,\n",
    "    \"inter_step_drop_prob\": 0.0,\n",
    "    \"num_nodes\": data.num_nodes,\n",
    "    \"num_edges\": data.num_edges,\n",
    "    \"device\": str(DEVICE),\n",
    "}\n",
    "with open(os.path.join(RESULTS_DIR, f\"config_{RUN_ID}.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "train_loader = NeighborLoader(\n",
    "    data.cpu(),\n",
    "    num_neighbors=[25, 15], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_nodes=data.train_mask,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "encoder = GCL_Encoder(data.num_features, HIDDEN_CHANNELS, OUT_CHANNELS).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=LR_PRETRAIN)\n",
    "augmentor = TemporalAugmentor(intra_step_drop_prob=0.5, inter_step_drop_prob=0.0)\n",
    "\n",
    "def contrastive_loss(z1, z2, temperature=0.1):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    sim_matrix = torch.mm(z1, z2.t()) / temperature\n",
    "    labels = torch.arange(z1.size(0)).to(DEVICE)\n",
    "    return F.cross_entropy(sim_matrix, labels)\n",
    "\n",
    "print(\"Starting Unsupervised Pre-training...\")\n",
    "encoder.train()\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS_PRETRAIN + 1):\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        \n",
    "        view1 = augmentor.get_view(batch, mode='temporal_edge')\n",
    "        view2 = augmentor.get_view(batch, mode='feature')\n",
    "        \n",
    "        _, z1 = encoder(view1.x, view1.edge_index, view1.edge_attr)\n",
    "        _, z2 = encoder(view2.x, view2.edge_index, view2.edge_attr)\n",
    "        \n",
    "        loss = contrastive_loss(z1, z2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "    \n",
    "    avg_loss = total_loss / steps\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch:03d} | Contrastive Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        checkpoint_path = os.path.join(RESULTS_DIR, f\"encoder_checkpoint_epoch{epoch}_{RUN_ID}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss_history': loss_history,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "print(\"Pre-training complete.\")\n",
    "\n",
    "final_model_path = os.path.join(RESULTS_DIR, f\"encoder_final_{RUN_ID}.pt\")\n",
    "torch.save({\n",
    "    'epoch': EPOCHS_PRETRAIN,\n",
    "    'model_state_dict': encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss_history': loss_history,\n",
    "    'config': config,\n",
    "}, final_model_path)\n",
    "print(f\"Final encoder saved: {final_model_path}\")\n",
    "\n",
    "loss_df = pd.DataFrame({'epoch': range(1, len(loss_history) + 1), 'loss': loss_history})\n",
    "loss_df.to_csv(os.path.join(RESULTS_DIR, f\"loss_history_{RUN_ID}.csv\"), index=False)\n",
    "print(\"Loss history saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_EVAL = 0.001\n",
    "EPOCHS_EVAL = 100\n",
    "\n",
    "print(\"Generating node embeddings for the whole graph...\")\n",
    "encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        embeddings, _ = encoder(data.x, data.edge_index, data.edge_attr)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU OOM: {e}\")\n",
    "        print(\"Switching encoder to CPU for inference...\")\n",
    "        encoder_cpu = encoder.cpu()\n",
    "        data_cpu = data.cpu()\n",
    "        embeddings, _ = encoder_cpu(data_cpu.x, data_cpu.edge_index, data_cpu.edge_attr)\n",
    "        embeddings = embeddings.to(DEVICE)\n",
    "        encoder.to(DEVICE)\n",
    "\n",
    "print(\"Embedding Statistics:\")\n",
    "print(f\"  Shape: {embeddings.shape}\")\n",
    "print(f\"  Mean: {embeddings.mean().item():.4f}\")\n",
    "print(f\"  Std: {embeddings.std().item():.4f}\")\n",
    "print(f\"  Min: {embeddings.min().item():.4f}\")\n",
    "print(f\"  Max: {embeddings.max().item():.4f}\")\n",
    "print(f\"  Has NaN: {torch.isnan(embeddings).any().item()}\")\n",
    "print(f\"  Has Inf: {torch.isinf(embeddings).any().item()}\")\n",
    "\n",
    "if torch.isnan(embeddings).any() or torch.isinf(embeddings).any():\n",
    "    print(\"Sanitizing embeddings (replacing NaN/Inf with 0)...\")\n",
    "    embeddings = torch.nan_to_num(embeddings, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "embeddings = F.normalize(embeddings, dim=1)\n",
    "\n",
    "embeddings_path = os.path.join(RESULTS_DIR, f\"embeddings_{RUN_ID}.pt\")\n",
    "torch.save(embeddings.cpu(), embeddings_path)\n",
    "print(f\"Embeddings saved: {embeddings_path}\")\n",
    "\n",
    "X = embeddings.detach()\n",
    "y = data.y.to(DEVICE)\n",
    "\n",
    "labeled_mask = (y != -1)\n",
    "train_mask = data.train_mask.to(DEVICE) & labeled_mask\n",
    "test_mask = data.test_mask.to(DEVICE) & labeled_mask\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "train_class_counts = torch.bincount(y_train)\n",
    "print(\"Training Class Distribution:\")\n",
    "print(f\"  Class 0 (Licit): {train_class_counts[0].item():,}\")\n",
    "print(f\"  Class 1 (Illicit): {train_class_counts[1].item():,}\")\n",
    "\n",
    "class_weights = 1.0 / train_class_counts.float()\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "print(f\"  Class Weights: {class_weights.tolist()}\")\n",
    "\n",
    "classifier = LogisticRegression(X.shape[1], 2).to(DEVICE)\n",
    "optimizer_eval = torch.optim.Adam(classifier.parameters(), lr=LR_EVAL)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
    "\n",
    "print(f\"Training Linear Classifier on {len(y_train):,} nodes...\")\n",
    "\n",
    "best_f1 = 0\n",
    "eval_history = []\n",
    "for epoch in range(1, EPOCHS_EVAL + 1):\n",
    "    classifier.train()\n",
    "    optimizer_eval.zero_grad()\n",
    "    \n",
    "    out = classifier(X_train)\n",
    "    loss = criterion(out, y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer_eval.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = classifier(X_test)\n",
    "            val_preds = val_out.argmax(dim=1)\n",
    "            val_f1 = f1_score(y_test.cpu().numpy(), val_preds.cpu().numpy(), pos_label=1)\n",
    "            eval_history.append({'epoch': epoch, 'loss': loss.item(), 'val_f1': val_f1})\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "        print(f\"  Epoch {epoch:3d} | Loss: {loss.item():.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    out_test = classifier(X_test)\n",
    "    probs = F.softmax(out_test, dim=1)\n",
    "    preds = probs.argmax(dim=1)\n",
    "    \n",
    "    y_true = y_test.cpu().numpy()\n",
    "    y_pred = preds.cpu().numpy()\n",
    "    y_prob = probs[:, 1].cpu().numpy()\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='binary', pos_label=1)\n",
    "    prec = precision_score(y_true, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='binary', pos_label=1, zero_division=0)\n",
    "    acc = (y_true == y_pred).sum() / len(y_true)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(\"Evaluation\")\n",
    "    print(f\"  Accuracy:       {acc:.4f}\")\n",
    "    print(f\"  Precision (1):  {prec:.4f}\")\n",
    "    print(f\"  Recall (1):     {rec:.4f}\")\n",
    "    print(f\"  F1 Score (1):   {f1:.4f}\")\n",
    "    print(f\"  F1 Macro:       {f1_macro:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"  [[TN={cm[0,0]:5d}, FP={cm[0,1]:5d}]\")\n",
    "    print(f\"   [FN={cm[1,0]:5d}, TP={cm[1,1]:5d}]]\")\n",
    "    print(\"\\nPer-Class Breakdown:\")\n",
    "    print(f\"  Licit (0):   {cm[0,0]:,} correct / {cm[0].sum():,} total = {cm[0,0]/cm[0].sum():.2%}\")\n",
    "    print(f\"  Illicit (1): {cm[1,1]:,} correct / {cm[1].sum():,} total = {cm[1,1]/cm[1].sum():.2%}\")\n",
    "\n",
    "classifier_path = os.path.join(RESULTS_DIR, f\"classifier_{RUN_ID}.pt\")\n",
    "torch.save(classifier.state_dict(), classifier_path)\n",
    "print(f\"\\nClassifier saved: {classifier_path}\")\n",
    "\n",
    "results = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"accuracy\": float(acc),\n",
    "    \"precision\": float(prec),\n",
    "    \"recall\": float(rec),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"f1_macro\": float(f1_macro),\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "    \"train_samples\": int(len(y_train)),\n",
    "    \"test_samples\": int(len(y_test)),\n",
    "    \"class_distribution_train\": {\n",
    "        \"licit\": int(train_class_counts[0].item()),\n",
    "        \"illicit\": int(train_class_counts[1].item())\n",
    "    },\n",
    "    \"eval_history\": eval_history,\n",
    "}\n",
    "\n",
    "results_path = os.path.join(RESULTS_DIR, f\"results_{RUN_ID}.json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved: {results_path}\")\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'prob_illicit': y_prob,\n",
    "})\n",
    "predictions_path = os.path.join(RESULTS_DIR, f\"predictions_{RUN_ID}.csv\")\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "print(f\"Predictions saved: {predictions_path}\")\n",
    "\n",
    "print(f\"ALL FILES SAVED TO: {RESULTS_DIR}\")\n",
    "print(f\"  - config_{RUN_ID}.json\")\n",
    "print(f\"  - encoder_final_{RUN_ID}.pt\")\n",
    "print(f\"  - loss_history_{RUN_ID}.csv\")\n",
    "print(f\"  - embeddings_{RUN_ID}.pt\")\n",
    "print(f\"  - classifier_{RUN_ID}.pt\")\n",
    "print(f\"  - results_{RUN_ID}.json\")\n",
    "print(f\"  - predictions_{RUN_ID}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitcoin_fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
