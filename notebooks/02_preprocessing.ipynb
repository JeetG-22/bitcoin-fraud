{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eceda274",
   "metadata": {},
   "source": [
    "### Prepare Data from Raw Data\n",
    "The Elliptic dataset contains 3 raw files:\n",
    "1. `elliptic_txs_classes.csv`: Each row contains a single transaction with a unique `txId` and a `class` value (1 = illicit, 2 = licit, unknown = unknown)\n",
    "1. `elliptic_txs_edgelist.csv`: Each row is an edge representing that Bitcoin from one transaction has been used as input of another transaction.\n",
    "1. `elliptic_txs_features.csv`: Each row maps 1-1 to the class list and contains more extensive features (166 total). No headers are provided.\n",
    "\n",
    "Here, we combined the classes and features dataset into one, and we also construct an adjacency list representation for\n",
    "easier parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a622be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7579c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath(\"../elliptic_bitcoin_dataset/\")\n",
    "output_path = os.path.abspath(\"../02_data\")\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    raise Exception(\"First download elliptic bitcoin dataset\")\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff32f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes Data: (203769, 1)\n",
      "Features Data: (203769, 166)\n",
      "Edges Data: (234355, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read in raw data\n",
    "classes = (\n",
    "    pd.read_csv(os.path.join(base_path, \"elliptic_txs_classes.csv\")).set_index(\n",
    "        \"txId\",\n",
    "    )\n",
    "    # Replace class labels with something more descriptive\n",
    "    .replace({\"1\": \"illicit\", \"2\": \"licit\", \"unknown\": \"unknown\"})\n",
    ")\n",
    "features = pd.read_csv(\n",
    "    os.path.join(base_path, \"elliptic_txs_features.csv\"),\n",
    "    index_col=\"txId\",\n",
    "    # The dataset does not include column names. The paper describes the following features:\n",
    "    # a unique transaction id, 94 local features (including a time step), and 72 aggregate features.\n",
    "    names=[\n",
    "        \"txId\",\n",
    "        \"timeStep\",\n",
    "        *[f\"local{i + 1}\" for i in range(94)][:-1],  # already accounted for timeStep\n",
    "        *[f\"aggregate{i + 1}\" for i in range(72)],\n",
    "    ],\n",
    ")\n",
    "edges = pd.read_csv(\n",
    "    os.path.join(base_path, \"elliptic_txs_edgelist.csv\"),\n",
    "    header=0,\n",
    "    names=[\"fromTxId\", \"toTxId\"],\n",
    ")\n",
    "\n",
    "print(\"Classes Data:\", classes.shape)\n",
    "print(\"Features Data:\", features.shape)\n",
    "print(\"Edges Data:\", edges.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02eeeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the classes and the features into a single dataset.\n",
    "joined = pd.concat([classes, features], axis=1)\n",
    "# Split data for training and testing\n",
    "train, test = train_test_split(\n",
    "    joined, train_size=0.75, stratify=joined[\"class\"], random_state=42\n",
    ")\n",
    "\n",
    "# These are large files, so saving these CSVs takes some time\n",
    "joined.to_csv(os.path.join(output_path, \"transactions.csv\"))\n",
    "train.to_csv(os.path.join(output_path, \"transactions_train.csv\"))\n",
    "test.to_csv(os.path.join(output_path, \"transactions_test.csv\"))\n",
    "edges.to_csv(os.path.join(output_path, \"edges.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6284415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adjacency list (might use it later for visualization purposes)\n",
    "forward_links = edges.groupby(\"fromTxId\")[\"toTxId\"].apply(\n",
    "    lambda x: \",\".join(map(str, x))\n",
    ")\n",
    "back_links = edges.groupby(\"toTxId\")[\"fromTxId\"].apply(lambda x: \",\".join(map(str, x)))\n",
    "\n",
    "pd.merge(\n",
    "    forward_links, back_links, how=\"outer\", left_index=True, right_index=True\n",
    ").to_csv(\n",
    "    os.path.join(output_path, \"adjacency_list.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    index_label=\"txId\",\n",
    "    header=[\"forward\", \"backward\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c99d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
