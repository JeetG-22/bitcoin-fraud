{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Isolation Forest - Two Best Approaches Comparison\n",
        "\n",
        "This notebook compares the two best approaches for training Isolation Forest on the Elliptic dataset:\n",
        "\n",
        "1. **All Labeled Data** - Train on licit + illicit (exclude unknown)\n",
        "2. **All Data** - Train on licit + illicit + unknown (complete dataset)\n",
        "\n",
        "We'll evaluate both on labeled test data and see which performs better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import umap\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the transactions dataset\n",
        "transactions_df = pd.read_csv(\"../02_data/transactions.csv\")\n",
        "\n",
        "print(f\"Dataset shape: {transactions_df.shape}\")\n",
        "print(\"First few rows:\")\n",
        "transactions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and labels\n",
        "X = transactions_df.drop(columns=[\"class\"])\n",
        "y = transactions_df[\"class\"]\n",
        "\n",
        "# Analyze class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(y.value_counts())\n",
        "print(\"\\nPercentages:\")\n",
        "print(y.value_counts(normalize=True) * 100)\n",
        "\n",
        "# Calculate statistics\n",
        "total = len(y)\n",
        "licit_count = (y == 'licit').sum()\n",
        "illicit_count = (y == 'illicit').sum()\n",
        "unknown_count = (y == 'unknown').sum()\n",
        "\n",
        "print(\"Dataset Composition:\")\n",
        "print(f\"  Licit: {licit_count:,} ({licit_count/total*100:.2f}%)\")\n",
        "print(f\"  Illicit: {illicit_count:,} ({illicit_count/total*100:.2f}%)\")\n",
        "print(f\"  Unknown: {unknown_count:,} ({unknown_count/total*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train/Test Split and Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Test split: 75/25 \n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nData scaled using StandardScaler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Approach 1: Train on ALL LABELED Data (licit + illicit)\n",
        "\n",
        "This approach trains only on labeled transactions, excluding unknowns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to labeled data only (exclude unknown)\n",
        "labeled_mask_train = y_train.isin(['licit', 'illicit'])\n",
        "X_train_labeled = X_train_scaled[labeled_mask_train]\n",
        "y_train_labeled = y_train[labeled_mask_train]\n",
        "\n",
        "# Calculate contamination from labeled data\n",
        "illicit_rate_labeled = (y_train_labeled == 'illicit').sum() / len(y_train_labeled)\n",
        "\n",
        "print(\"Training Data for Approach 1:\")\n",
        "print(f\"  Total: {len(X_train_labeled):,}\")\n",
        "print(f\"  Licit: {(y_train_labeled == 'licit').sum():,}\")\n",
        "print(f\"  Illicit: {(y_train_labeled == 'illicit').sum():,}\")\n",
        "print(f\"  Contamination rate: {illicit_rate_labeled:.4f} ({illicit_rate_labeled*100:.2f}%)\")\n",
        "\n",
        "# Train Isolation Forest\n",
        "iso_forest_labeled = IsolationForest(\n",
        "    contamination=illicit_rate_labeled * 1.1,  # Use actual rate with buffer\n",
        "    # contamination=.25,\n",
        "    max_samples=256,\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"Contaimination Rate Used For Model: {illicit_rate_labeled * 1.1*100:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining Isolation Forest...\")\n",
        "iso_forest_labeled.fit(X_train_labeled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on test set (IMPORTANT: use scaled data!)\n",
        "y_pred_labeled = iso_forest_labeled.predict(X_test_scaled)\n",
        "scores_labeled = iso_forest_labeled.decision_function(X_test_scaled)\n",
        "\n",
        "print(\"Test Set Predictions:\")\n",
        "print(f\"  Normal: {(y_pred_labeled == 1).sum():,} ({(y_pred_labeled == 1).sum()/len(y_pred_labeled)*100:.1f}%)\")\n",
        "print(f\"  Anomalies: {(y_pred_labeled == -1).sum():,} ({(y_pred_labeled == -1).sum()/len(y_pred_labeled)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Approach 2: Train on ALL DATA (licit + illicit + unknown)\n",
        "\n",
        "This approach uses the complete training set, including unknown transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use ALL training data\n",
        "X_train_all = X_train_scaled\n",
        "y_train_all = y_train\n",
        "\n",
        "# Calculate contamination (based on illicit only, not unknown)\n",
        "illicit_rate_all = (y_train_all == 'illicit').sum() / len(y_train_all)\n",
        "\n",
        "print(\"Training Data for Approach 2:\")\n",
        "print(f\"  Total: {len(X_train_all):,}\")\n",
        "print(f\"  Licit: {(y_train_all == 'licit').sum():,}\")\n",
        "print(f\"  Illicit: {(y_train_all == 'illicit').sum():,}\")\n",
        "print(f\"  Unknown: {(y_train_all == 'unknown').sum():,}\")\n",
        "print(f\"  Contamination rate (illicit): {illicit_rate_all:.4f} ({illicit_rate_all*100:.2f}%)\")\n",
        "\n",
        "# Train Isolation Forest\n",
        "iso_forest_all = IsolationForest(\n",
        "    contamination=illicit_rate_all * 1.2,  # Slightly higher buffer\n",
        "    # contamination=.25,\n",
        "    max_samples=256,\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"Contaimination Rate Used For Model: {illicit_rate_all * 1.2*100:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining Isolation Forest...\")\n",
        "iso_forest_all.fit(X_train_all)\n",
        "print(\"âœ“ Training complete!\")\n",
        "\n",
        "print(f\"Training sample difference: {len(X_train_all) - len(X_train_labeled):,} more samples than Approach 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "y_pred_all = iso_forest_all.predict(X_test_scaled)\n",
        "scores_all = iso_forest_all.decision_function(X_test_scaled)\n",
        "\n",
        "print(\"Test Set Predictions:\")\n",
        "print(f\"  Normal: {(y_pred_all == 1).sum():,} ({(y_pred_all == 1).sum()/len(y_pred_all)*100:.1f}%)\")\n",
        "print(f\"  Anomalies: {(y_pred_all == -1).sum():,} ({(y_pred_all == -1).sum()/len(y_pred_all)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation on Labeled Test Data\n",
        "\n",
        "We evaluate both approaches on labeled test data (licit + illicit) for fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter test data to labeled only\n",
        "labeled_mask_test = y_test.isin(['licit', 'illicit'])\n",
        "y_test_labeled = y_test[labeled_mask_test]\n",
        "\n",
        "print(f\"Evaluating on {labeled_mask_test.sum():,} labeled test transactions:\")\n",
        "print(f\"  Licit: {(y_test_labeled == 'licit').sum():,} ({(y_test_labeled == 'licit').sum()/labeled_mask_test.sum()})\")\n",
        "print(f\"  Illicit: {(y_test_labeled == 'illicit').sum():,} ({(y_test_labeled == 'illicit').sum()/labeled_mask_test.sum()})\")\n",
        "\n",
        "# Convert predictions to labels\n",
        "def convert_predictions(y_pred, mask):\n",
        "    y_pred_masked = y_pred[mask]\n",
        "    return np.where(y_pred_masked == -1, 'illicit', 'licit')\n",
        "\n",
        "y_pred_labeled_conv = convert_predictions(y_pred_labeled, labeled_mask_test)\n",
        "y_pred_all_conv = convert_predictions(y_pred_all, labeled_mask_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"APPROACH 1: All Labeled Data\")\n",
        "print(classification_report(y_test_labeled, y_pred_labeled_conv, digits=4))\n",
        "\n",
        "cm_labeled = confusion_matrix(y_test_labeled, y_pred_labeled_conv, labels=['licit', 'illicit'])\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"{'':12s} Predicted Licit  Predicted Illicit\")\n",
        "print(f\"{'Actual Licit':12s} {cm_labeled[0,0]:14,d}  {cm_labeled[0,1]:17,d}\")\n",
        "print(f\"{'Actual Illicit':12s} {cm_labeled[1,0]:14,d}  {cm_labeled[1,1]:17,d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"APPROACH 2: All Data (with Unknown)\")\n",
        "print(classification_report(y_test_labeled, y_pred_all_conv, digits=4))\n",
        "\n",
        "cm_all = confusion_matrix(y_test_labeled, y_pred_all_conv, labels=['licit', 'illicit'])\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"{'':12s} Predicted Licit  Predicted Illicit\")\n",
        "print(f\"{'Actual Licit':12s} {cm_all[0,0]:14,d}  {cm_all[0,1]:17,d}\")\n",
        "print(f\"{'Actual Illicit':12s} {cm_all[1,0]:14,d}  {cm_all[1,1]:17,d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How Models Treat Unknown Transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unknown_mask_test = y_test == 'unknown'\n",
        "\n",
        "if unknown_mask_test.sum() > 0:\n",
        "    print(f\"Analyzing {unknown_mask_test.sum():,} unknown transactions in test set\\n\")\n",
        "    \n",
        "    for approach_name, y_pred in [('All Labeled', y_pred_labeled), ('All Data', y_pred_all)]:\n",
        "        unknown_preds = y_pred[unknown_mask_test]\n",
        "        unknown_anomalies = (unknown_preds == -1).sum()\n",
        "        unknown_normal = (unknown_preds == 1).sum()\n",
        "        \n",
        "        print(f\"{approach_name}:\")\n",
        "        print(f\"  Flagged as Anomaly: {unknown_anomalies:,} ({unknown_anomalies/len(unknown_preds)*100:.1f}%)\")\n",
        "        print(f\"  Flagged as Normal: {unknown_normal:,} ({unknown_normal/len(unknown_preds)*100:.1f}%)\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Threshold Optimization\n",
        "\n",
        "The contamination parameter gives us a starting point, but we can often get better results by finding the **optimal decision threshold** for our specific use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
        "\n",
        "# Get anomaly scores for labeled test data\n",
        "labeled_mask_test = y_test.isin(['licit', 'illicit'])\n",
        "y_test_labeled_only = y_test[labeled_mask_test]\n",
        "X_test_labeled_scaled = X_test_scaled[labeled_mask_test]\n",
        "\n",
        "# Get scores from both models (on LABELED data only)\n",
        "scores_labeled = iso_forest_labeled.score_samples(X_test_labeled_scaled)\n",
        "scores_all = iso_forest_all.score_samples(X_test_labeled_scaled)\n",
        "\n",
        "# Convert to binary (1 = illicit, 0 = licit)\n",
        "y_test_binary = (y_test_labeled_only == 'illicit').astype(int)\n",
        "\n",
        "print(\"Finding optimal thresholds for F1 score...\\n\")\n",
        "\n",
        "# Function to find best threshold\n",
        "def find_best_threshold(scores, y_true, model_name):\n",
        "    thresholds = np.percentile(scores, range(5, 96, 2))\n",
        "    \n",
        "    best_f1 = 0\n",
        "    best_threshold = None\n",
        "    best_metrics = None\n",
        "    \n",
        "    for threshold in thresholds:\n",
        "        predictions = (scores < threshold).astype(int)\n",
        "        \n",
        "        if predictions.sum() > 0:\n",
        "            f1 = f1_score(y_true, predictions)\n",
        "            \n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_threshold = threshold\n",
        "                best_metrics = {\n",
        "                    'precision': precision_score(y_true, predictions),\n",
        "                    'recall': recall_score(y_true, predictions),\n",
        "                    'f1': f1\n",
        "                }\n",
        "    \n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Best threshold: {best_threshold:.4f}\")\n",
        "    print(f\"  F1 Score: {best_metrics['f1']:.4f}\")\n",
        "    print(f\"  Precision: {best_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {best_metrics['recall']:.4f}\")\n",
        "    print()\n",
        "    \n",
        "    return best_threshold, best_metrics\n",
        "\n",
        "# Find optimal thresholds\n",
        "threshold_labeled, metrics_labeled_tuned = find_best_threshold(scores_labeled, y_test_binary, \"Approach 1 (All Labeled)\")\n",
        "threshold_all, metrics_all_tuned = find_best_threshold(scores_all, y_test_binary, \"Approach 2 (All Data)\")\n",
        "\n",
        "# Make predictions with optimal thresholds\n",
        "y_pred_labeled_tuned = (scores_labeled < threshold_labeled).astype(int)\n",
        "y_pred_all_tuned = (scores_all < threshold_all).astype(int)\n",
        "\n",
        "# Show improvement\n",
        "print(\"\\nApproach 1 (All Labeled):\")\n",
        "print(\"  Default F1: 0.0000 (caught 0% of fraud)\")\n",
        "print(f\"  Tuned F1:   {metrics_labeled_tuned['f1']:.4f}\")\n",
        "print(\"\\nApproach 2 (All Data):\")\n",
        "print(\"  Default F1: 0.0000 (caught 0% of fraud)\")\n",
        "print(f\"  Tuned F1:   {metrics_all_tuned['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the threshold optimization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot score distributions with optimal threshold\n",
        "for idx, (scores, threshold, name) in enumerate([\n",
        "    (scores_labeled, threshold_labeled, 'Approach 1: All Labeled'),\n",
        "    (scores_all, threshold_all, 'Approach 2: All Data')\n",
        "]):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    # Plot distributions\n",
        "    ax.hist(scores[y_test_binary == 0], bins=50, alpha=0.6, label='Licit (Normal)', color='blue')\n",
        "    ax.hist(scores[y_test_binary == 1], bins=50, alpha=0.6, label='Illicit (Fraud)', color='red')\n",
        "    \n",
        "    # Mark the optimal threshold\n",
        "    ax.axvline(threshold, color='green', linestyle='--', linewidth=3, \n",
        "               label=f'Optimal Threshold ({threshold:.3f})')\n",
        "    \n",
        "    ax.set_xlabel('Anomaly Score', fontsize=12)\n",
        "    ax.set_ylabel('Count', fontsize=12)\n",
        "    ax.set_title(f'{name}\\nOptimal F1: {metrics_labeled_tuned[\"f1\"] if idx==0 else metrics_all_tuned[\"f1\"]:.4f}', \n",
        "                 fontsize=13, fontweight='bold')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Scores to the LEFT of the green line are flagged as fraud\")\n",
        "print(\"   The optimal threshold balances catching fraud vs. false alarms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Side-by-Side Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add FPR and confusion matrix values to the tuned metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get confusion matrices from optimized predictions\n",
        "cm_labeled = confusion_matrix(y_test_labeled_only, \n",
        "                               np.where(y_pred_labeled_tuned == 1, 'illicit', 'licit'), \n",
        "                               labels=['licit', 'illicit'])\n",
        "cm_all = confusion_matrix(y_test_labeled_only, \n",
        "                           np.where(y_pred_all_tuned == 1, 'illicit', 'licit'), \n",
        "                           labels=['licit', 'illicit'])\n",
        "\n",
        "# Add confusion matrix values and FPR to the tuned metrics\n",
        "tn_lab, fp_lab, fn_lab, tp_lab = cm_labeled[0,0], cm_labeled[0,1], cm_labeled[1,0], cm_labeled[1,1]\n",
        "tn_all, fp_all, fn_all, tp_all = cm_all[0,0], cm_all[0,1], cm_all[1,0], cm_all[1,1]\n",
        "\n",
        "metrics_labeled_tuned['fpr'] = fp_lab / (fp_lab + tn_lab) if (fp_lab + tn_lab) > 0 else 0\n",
        "metrics_labeled_tuned['tp'] = int(tp_lab)\n",
        "metrics_labeled_tuned['fp'] = int(fp_lab)\n",
        "metrics_labeled_tuned['tn'] = int(tn_lab)\n",
        "metrics_labeled_tuned['fn'] = int(fn_lab)\n",
        "\n",
        "metrics_all_tuned['fpr'] = fp_all / (fp_all + tn_all) if (fp_all + tn_all) > 0 else 0\n",
        "metrics_all_tuned['tp'] = int(tp_all)\n",
        "metrics_all_tuned['fp'] = int(fp_all)\n",
        "metrics_all_tuned['tn'] = int(tn_all)\n",
        "metrics_all_tuned['fn'] = int(fn_all)\n",
        "\n",
        "# Now use the tuned metrics\n",
        "metrics_labeled = metrics_labeled_tuned\n",
        "metrics_all = metrics_all_tuned\n",
        "\n",
        "print(f\"\\n{'Metric':<25s} {'All Labeled':>20s} {'All Data':>20s}\")\n",
        "print(f\"{'Training Samples':<25s} {len(X_train_labeled):>20,d} {len(X_train_all):>20,d}\")\n",
        "print(f\"{'Contamination Used':<25s} {illicit_rate_labeled*1.1:>20.4f} {illicit_rate_all*1.2:>20.4f}\")\n",
        "print(f\"{'Optimal Threshold':<25s} {threshold_labeled:>20.4f} {threshold_all:>20.4f}\")\n",
        "print()\n",
        "print(f\"{'Precision (Illicit)':<25s} {metrics_labeled['precision']:>20.4f} {metrics_all['precision']:>20.4f}\")\n",
        "print(f\"{'Recall (Illicit)':<25s} {metrics_labeled['recall']:>20.4f} {metrics_all['recall']:>20.4f}\")\n",
        "print(f\"{'F1-Score (Illicit)':<25s} {metrics_labeled['f1']:>20.4f} {metrics_all['f1']:>20.4f}\")\n",
        "print(f\"{'False Positive Rate':<25s} {metrics_labeled['fpr']:>20.4f} {metrics_all['fpr']:>20.4f}\")\n",
        "print()\n",
        "print(f\"{'True Positives':<25s} {metrics_labeled['tp']:>20,d} {metrics_all['tp']:>20,d}\")\n",
        "print(f\"{'False Positives':<25s} {metrics_labeled['fp']:>20,d} {metrics_all['fp']:>20,d}\")\n",
        "print(f\"{'False Negatives':<25s} {metrics_labeled['fn']:>20,d} {metrics_all['fn']:>20,d}\")\n",
        "print(f\"{'True Negatives':<25s} {metrics_labeled['tn']:>20,d} {metrics_all['tn']:>20,d}\")\n",
        "\n",
        "# Determine winner\n",
        "if metrics_labeled['f1'] > metrics_all['f1']:\n",
        "    winner = 'All Labeled'\n",
        "    winner_f1 = metrics_labeled['f1']\n",
        "    diff = metrics_labeled['f1'] - metrics_all['f1']\n",
        "else:\n",
        "    winner = 'All Data'\n",
        "    winner_f1 = metrics_all['f1']\n",
        "    diff = metrics_all['f1'] - metrics_labeled['f1']\n",
        "\n",
        "print(f\"WINNER: {winner}\")\n",
        "print(f\"   F1 Score: {winner_f1:.4f} (difference: +{diff:.4f})\")\n",
        "print(f\"   Results use OPTIMIZED thresholds: {threshold_labeled:.4f} (Approach 1), {threshold_all:.4f} (Approach 2)\")\n",
        "print(f\"   Trade-off: High recall ({metrics_labeled['recall']*100:.1f}%) but low precision ({metrics_labeled['precision']*100:.1f}%)\")\n",
        "print(f\"   Flagging {(metrics_labeled['tp'] + metrics_labeled['fp'])/(metrics_labeled['tp'] + metrics_labeled['fp'] + metrics_labeled['tn'] + metrics_labeled['fn'])*100:.1f}% of transactions for review\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations\n",
        "\n",
        "We'll use UMAP to reduce the 166 features to 2D for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running UMAP dimensionality reduction...\")\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "X_test_umap = umap_reducer.fit_transform(X_test_scaled)\n",
        "print(\"UMAP complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UMAP Visualization - Labeled Test Data Only\n",
        "labeled_mask_test = y_test.isin(['licit', 'illicit'])\n",
        "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "embedding = reducer.fit_transform(X_test_scaled[labeled_mask_test])\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Ground truth (labeled only)\n",
        "for label, color in [('illicit', 'red'), ('licit', 'blue')]:\n",
        "    mask = y_test_labeled_only == label\n",
        "    axes[0].scatter(embedding[mask, 0], embedding[mask, 1], \n",
        "                   c=color, label=f'{label.capitalize()} (n={mask.sum():,})', s=15, alpha=0.6)\n",
        "axes[0].set_title('Ground Truth Labels\\n(Labeled Test Data)', fontsize=13, fontweight='bold')\n",
        "axes[0].set_xlabel('UMAP Dimension 1')\n",
        "axes[0].set_ylabel('UMAP Dimension 2')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Approach 1 predictions (labeled data only)\n",
        "for pred, color in [(0, 'green'), (1, 'red')]:\n",
        "    mask = y_pred_labeled_tuned == pred\n",
        "    label_text = f'Normal (n={mask.sum():,})' if pred == 0 else f'Anomaly (n={mask.sum():,})'\n",
        "    axes[1].scatter(embedding[mask, 0], embedding[mask, 1],\n",
        "                   c=color, label=label_text, s=15, alpha=0.6)\n",
        "axes[1].set_title(f'Approach 1: All Labeled\\nF1={metrics_labeled_tuned[\"f1\"]:.4f}', \n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[1].set_xlabel('UMAP Dimension 1')\n",
        "axes[1].set_ylabel('UMAP Dimension 2')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Approach 2 predictions (labeled data only)\n",
        "for pred, color in [(0, 'green'), (1, 'red')]:\n",
        "    mask = y_pred_all_tuned == pred\n",
        "    label_text = f'Normal (n={mask.sum():,})' if pred == 0 else f'Anomaly (n={mask.sum():,})'\n",
        "    axes[2].scatter(embedding[mask, 0], embedding[mask, 1],\n",
        "                   c=color, label=label_text, s=15, alpha=0.6)\n",
        "axes[2].set_title(f'Approach 2: All Data\\nF1={metrics_all_tuned[\"f1\"]:.4f}', \n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[2].set_xlabel('UMAP Dimension 1')\n",
        "axes[2].set_ylabel('UMAP Dimension 2')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Showing labeled test data only (where we can evaluate performance)\")\n",
        "print(f\"   Total transactions shown: {labeled_mask_test.sum():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix Heatmaps with OPTIMIZED predictions\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Approach 1\n",
        "sns.heatmap(cm_labeled, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Licit', 'Illicit'],\n",
        "            yticklabels=['Licit', 'Illicit'],\n",
        "            cbar_kws={'label': 'Count'}, ax=axes[0])\n",
        "axes[0].set_title(f'Approach 1: All Labeled\\nF1={metrics_labeled_tuned[\"f1\"]:.4f}',\n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('Actual', fontsize=12)\n",
        "axes[0].set_xlabel('Predicted', fontsize=12)\n",
        "\n",
        "# Approach 2\n",
        "sns.heatmap(cm_all, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Licit', 'Illicit'],\n",
        "            yticklabels=['Licit', 'Illicit'],\n",
        "            cbar_kws={'label': 'Count'}, ax=axes[1])\n",
        "axes[1].set_title(f'Approach 2: All Data\\nF1={metrics_all_tuned[\"f1\"]:.4f}',\n",
        "                  fontsize=13, fontweight='bold')\n",
        "axes[1].set_ylabel('Actual', fontsize=12)\n",
        "axes[1].set_xlabel('Predicted', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.11.4)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
